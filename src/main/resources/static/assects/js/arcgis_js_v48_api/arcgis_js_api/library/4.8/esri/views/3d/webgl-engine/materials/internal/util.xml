<?xml version="1.0" encoding="UTF-8"?>

<snippets>

<snippet name="alignToPixelCenter"><![CDATA[
  vec4 alignToPixelCenter(vec4 clipCoord, vec2 widthHeight) {
    // From clip space to (0 : 1), bias towards right pixel edge
    vec2 xy = vec2(.500123) + .5 * clipCoord.xy / clipCoord.w;

    // Size of a pixel in range (0 : 1)
    vec2 pixelSz = vec2(1.0) / widthHeight;

    // Round to nearest pixel center
    vec2 ij = (floor(xy * widthHeight) + vec2(0.5)) * pixelSz;

    // Convert back to clip space
    vec2 result = (ij * 2.0 - vec2(1.0)) * clipCoord.w;

    return vec4(result, clipCoord.zw);
  }
]]></snippet>

<snippet name="alignToPixelOrigin"><![CDATA[
  vec4 alignToPixelOrigin(vec4 clipCoord, vec2 widthHeight) {
    // From clip space to (0 : 1),
    vec2 xy = vec2(.5) + .5 * clipCoord.xy / clipCoord.w;

    // Size of a pixel in range (0 : 1)
    vec2 pixelSz = vec2(1.0) / widthHeight;

    // Round to nearest pixel border, (0 : 1)
    vec2 ij = floor((xy + .5 * pixelSz) * widthHeight) * pixelSz;

    // Convert back to clip space
    vec2 result = (ij * 2.0 - vec2(1.0)) * clipCoord.w;

    return vec4(result, clipCoord.zw);
  }
]]></snippet>

<snippet name="float2rgba"><![CDATA[
  // This is the maximum float value representable as 32bit fixed point,
  // it is rgba2float(vec4(1)) inlined.
  const float MAX_RGBA_FLOAT =
    255.0 / 256.0 +
    255.0 / 256.0 / 256.0 +
    255.0 / 256.0 / 256.0 / 256.0 +
    255.0 / 256.0 / 256.0 / 256.0 / 256.0;

  // Factors to convert to fixed point, i.e. factors (256^0, 256^1, 256^2, 256^3)
  const vec4 fixedPointFactors = vec4(1, 256, 256 * 256, 256 * 256 * 256);

  vec4 float2rgba(const float value) {
    // Make sure value is in the domain we can represent
    float valueInValidDomain = clamp(value, 0.0, MAX_RGBA_FLOAT);

    // Decompose value in 32bit fixed point parts represented as
    // uint8 rgba components. Decomposition uses the fractional part after multiplying
    // by a power of 256 (this removes the bits that are represented in the previous
    // component) and then converts the fractional part to 8bits.
    vec4 fixedPointU8 = floor(fract(valueInValidDomain * fixedPointFactors) * 256.0);

    // Convert uint8 values (from 0 to 255) to floating point representation for
    // the shader
    const float toU8AsFloat = 1.0 / 255.0;

    return fixedPointU8 * toU8AsFloat;
  }
]]></snippet>

<snippet name="rgba2float"><![CDATA[
  // Factors to convert rgba back to float
  const vec4 rgba2float_factors = vec4(
    255.0 / (256.0),
    255.0 / (256.0 * 256.0),
    255.0 / (256.0 * 256.0 * 256.0),
    255.0 / (256.0 * 256.0 * 256.0 * 256.0)
  );

  float rgba2float(vec4 rgba) {
    // Convert components from 0->1 back to 0->255 and then
    // add the components together with their corresponding
    // fixed point factors, i.e. (256^1, 256^2, 256^3, 256^4)
    return dot(rgba, rgba2float_factors);
  }
]]></snippet>

<snippet name="calcFragDepth"><![CDATA[
  float calcFragDepth(const in float depth) {
    //calc polygon offset
    const float SLOPE_SCALE = 2.0;
    const float BIAS = 2.0 * .000015259;    // 1 / (2^16 - 1)
    float m = max(abs(dFdx(depth)), abs(dFdy(depth)));
    float result = depth + SLOPE_SCALE * m + BIAS;
    return clamp(result, .0, .999999);
  }
]]></snippet>

<snippet name="evalShadow"><![CDATA[
  $rgba2float

  // "matrix" parameter used to have const qualifier as well, but IE11 couldn't deal with it at time of writing.
  // once IE11 is fine with it, const should probably be re-introduced
  float evalShadow(const in vec3 vpos, const in float depth, const in sampler2D depthTex, const int num, const in vec4 distance, in mat4 matrix[4], const in float halfPxSz) {
    //choose correct cascade
    int i = depth < distance[1] ? 0 : depth < distance[2] ? 1 : depth < distance[3] ? 2 : 3;

    if (i >= num) return .0;

    mat4 mat = i == 0 ? matrix[0] : i == 1 ? matrix[1] : i == 2 ? matrix[2] : matrix[3];

    vec4 lv = mat * vec4(vpos, 1.0);
    lv.xy /= lv.w;

    //vertex completely outside? -> no shadow
    vec3 lvpos = .5 * lv.xyz + vec3(.5);
    if (lvpos.z >= 1.0) return .0;
    if (lvpos.x < .0 || lvpos.x > 1.0 || lvpos.y < .0 || lvpos.y > 1.0) return .0;

    //calc coord in cascade texture
    vec2 uv = vec2(float(i - 2 * (i / 2)) *.5, float(i / 2) * .5) + .5 * lvpos.xy;

    float texSize = .5 / halfPxSz;

    //filter, offset by half pixels
    vec2 st = fract((vec2(halfPxSz) + uv) * texSize);

    float s00 = rgba2float(texture2D(depthTex, uv + vec2(-halfPxSz, -halfPxSz))) < lvpos.z ? 1.0 : .0;
    float s10 = rgba2float(texture2D(depthTex, uv + vec2(halfPxSz, -halfPxSz))) < lvpos.z ? 1.0 : .0;
    float s11 = rgba2float(texture2D(depthTex, uv + vec2(halfPxSz, halfPxSz))) < lvpos.z ? 1.0 : .0;
    float s01 = rgba2float(texture2D(depthTex, uv + vec2(-halfPxSz, halfPxSz))) < lvpos.z ? 1.0 : .0;

    return mix(mix(s00, s10, st.x), mix(s01, s11, st.x), st.y);
  }
]]></snippet>


<!--
  Scene Lighting Definitions:
  ================================================

  defines:
    - SH_ORDER: 1|2|3
  input:
    - normal: vec3
    - albedo: vec3
    - shadow: float
    - ssao: float
  return:
    - color: vec3
-->
<snippet name="sceneLightingDefinitions"><![CDATA[
  $viewingMode

  // main light
  /////////////////////////////////////////
  uniform vec3 lightingMainDirection;
  uniform vec3 lightingMainIntensity;

  // ambient lighting
  /////////////////////////////////////////
  #ifndef SH_ORDER
    #define SH_ORDER 2
  #endif

  #if SH_ORDER == 0
    uniform vec3 lightingAmbientSH0;
  #elif SH_ORDER == 1
    uniform vec4 lightingAmbientSH_R;
    uniform vec4 lightingAmbientSH_G;
    uniform vec4 lightingAmbientSH_B;
  #elif SH_ORDER == 2
    uniform vec3 lightingAmbientSH0;
    uniform vec4 lightingAmbientSH_R1;
    uniform vec4 lightingAmbientSH_G1;
    uniform vec4 lightingAmbientSH_B1;
    uniform vec4 lightingAmbientSH_R2;
    uniform vec4 lightingAmbientSH_G2;
    uniform vec4 lightingAmbientSH_B2;
  #endif

  // special tweaking
  //////////////////////////////////////////
  uniform float lightingFixedFactor;
  uniform float lightingGlobalFactor;

  uniform float ambientBoostFactor;

  // evaluation
  //////////////////////////////////////////

  vec3 evaluateSceneLighting(vec3 normal, vec3 albedo, float shadow, float ssao, vec3 additionalLight) {
    // evaluate the main light
    float dotVal = mix(clamp(-dot(normal, lightingMainDirection), 0.0, 1.0), 1.0, lightingFixedFactor);
    vec3 mainLight = (1.0 - shadow) * lightingMainIntensity * dotVal;

    // evaluate the sh ambient light
    #if SH_ORDER == 0
      vec3 ambientLight = 0.282095 * lightingAmbientSH0;
    #elif SH_ORDER == 1
      vec4 sh0 = vec4(
        0.282095,
        0.488603 * normal.x,
        0.488603 * normal.z,
        0.488603 * normal.y
      );
      vec3 ambientLight = vec3(
        dot(lightingAmbientSH_R, sh0),
        dot(lightingAmbientSH_G, sh0),
        dot(lightingAmbientSH_B, sh0)
      );
    #elif SH_ORDER == 2
      vec3 ambientLight = 0.282095 * lightingAmbientSH0;

      vec4 sh1 = vec4(
        0.488603 * normal.x,
        0.488603 * normal.z,
        0.488603 * normal.y,
        1.092548 * normal.x * normal.y
      );
      vec4 sh2 = vec4(
        1.092548 * normal.y * normal.z,
        0.315392 * (3.0 * normal.z * normal.z - 1.0),
        1.092548 * normal.x * normal.z,
        0.546274 * (normal.x * normal.x - normal.y * normal.y)
      );
      ambientLight += vec3(
        dot(lightingAmbientSH_R1, sh1),
        dot(lightingAmbientSH_G1, sh1),
        dot(lightingAmbientSH_B1, sh1)
      );
      ambientLight += vec3(
        dot(lightingAmbientSH_R2, sh2),
        dot(lightingAmbientSH_G2, sh2),
        dot(lightingAmbientSH_B2, sh2)
      );
    #endif
    ambientLight *= (1.0 - ssao);

    // inverse gamma correction on the albedo color
    float gamma = 2.1;
    vec3 albedoGammaC = pow(albedo, vec3(gamma));

    // physically correct BRDF normalizes by PI
    const float PI = 3.14159;
    vec3 totalLight = mainLight + ambientLight + additionalLight;
    totalLight = min(totalLight, vec3(PI, PI, PI));
    vec3 outColor = vec3((albedoGammaC / PI) * (totalLight));

    // apply gamma correction to the computed color
    outColor = pow(outColor, vec3(1.0/gamma));

    return outColor;
  }

]]></snippet>

<snippet name="sceneLightingAdditionalLightGlobal"><![CDATA[
  vec3 sceneLightingAdditionalLightGlobal(vec3 worldPos, float ssao, out float additionalAmbientScale) {
    // heuristic lighting model originally used in the terrain shading
    // now used to generated additional ambient light

#ifdef VIEWING_MODE_GLOBAL

      float vndl = -dot(normalize(worldPos), lightingMainDirection);

#else

      float vndl = -dot(vec3(0,0,1), lightingMainDirection);

#endif

    additionalAmbientScale = smoothstep(0.0, 1.0, clamp(vndl * 2.5, 0.0, 1.0));
    return ssao * lightingMainIntensity * additionalAmbientScale * ambientBoostFactor * lightingGlobalFactor;
  }
]]></snippet>

<snippet name="normal2envTC"><![CDATA[
  vec2 normal2envTC(vec3 normal) {
    float v = .5 + .5 * asin(normal.y) * 0.63661977;
    float u = .5 - .5 * atan(normal.z, normal.x) * 0.31830988;
    return vec2(u, v);
  }
]]></snippet>

<snippet name="vertexShaderShowDepth"><![CDATA[
  $vsprecisionf

  uniform mat4 proj;
  attribute vec2 $position;
  attribute vec2 $uv0;
  varying vec2 vtc;

  void main(void) {
    gl_Position = proj * vec4($position.x, $position.y, .0, 1.0);
    vtc = $uv0;
  }
]]></snippet>

  <snippet name="fragmentShaderShowDepth"><![CDATA[
  $fsprecisionf

  uniform sampler2D depthTex;
  varying vec2 vtc;
  $rgba2float
  void main() {
  // gl_FragColor = vec4(vec3(texture2D(depthTex, vtc).a), 1.0);
     gl_FragColor = vec4(rgba2float(texture2D(depthTex, vtc)));
  // gl_FragColor = texture2D(depthTex, vtc);
  }
]]></snippet>

<snippet name="vsUVQuad"><![CDATA[
  $vsprecisionf

  attribute vec2 $position;
  varying vec2 uv;

  void main(void) {
    gl_Position = vec4($position.x, $position.y, .0, 1.0);
    uv = $position * .5 + vec2(.5);
  }
]]></snippet>

<snippet name="toScreenCoords"><![CDATA[
  vec4 toScreenCoords(vec3 vertex) {
    vec4 vClipSpace = proj * view * vec4((model * vec4(vertex, 1.0)).xyz, 1.0);
    vClipSpace.xy *= screenSize;
    return vClipSpace/abs(vClipSpace.w);
  }
]]></snippet>

<snippet name="vvUniforms"><![CDATA[
#if defined(VV_SIZE)
  #define VV_CUSTOM_MODEL_MATRIX
#endif

#if defined(VV_SIZE)
  uniform vec3 vvSizeMinSize;
  uniform vec3 vvSizeMaxSize;
  uniform vec3 vvSizeOffset;
  uniform vec3 vvSizeFactor;
#elif defined(VV_CUSTOM_MODEL_MATRIX)
  uniform vec3 vvSizeValue;
#endif

#ifdef VV_CUSTOM_MODEL_MATRIX
  uniform mat3 vvSymbolRotation;
#endif

#ifdef VV_CUSTOM_MODEL_MATRIX
  uniform vec3 vvSymbolAnchor;
#endif

#ifdef VV_COLOR
  #define VV_COLOR_N 8
  uniform float vvColorValues[VV_COLOR_N];
  uniform vec4 vvColorColors[VV_COLOR_N];
#endif

]]></snippet>

<snippet name="vvFunctions"><![CDATA[
// Evaluation of size
#if defined(VV_SIZE)
  vec3 vvGetScale(vec4 featureAttribute) {
    return clamp(vvSizeOffset + featureAttribute.x * vvSizeFactor, vvSizeMinSize, vvSizeMaxSize);
  }
#elif defined(VV_CUSTOM_MODEL_MATRIX)
  vec3 vvGetScale(vec4 featureAttribute) {
    return vvSizeValue;
  }
#endif

// Applying the model matrix
#ifdef VV_CUSTOM_MODEL_MATRIX
  vec4 vvTransformPosition(vec3 position, vec4 featureAttribute) {
    return vec4(vvSymbolRotation * (vvGetScale(featureAttribute) * (position + vvSymbolAnchor)), 1.0);
  }

  vec4 vvTransformNormal(vec3 normal, vec4 featureAttribute) {
    // Normal transform is the inverse transpose of model transform
    return vec4(vvSymbolRotation * normal / vvGetScale(featureAttribute), 1.0);
  }
#endif

#ifdef VV_COLOR
  vec4 vvGetColor(vec4 featureAttribute, float values[VV_COLOR_N], vec4 colors[VV_COLOR_N]) {
    float value = featureAttribute.y;
    if (value <= values[0]) {
      return colors[0];
    }

    for (int i = 1; i < VV_COLOR_N; ++i) {
      if (values[i] >= value) {
        float f = (value - values[i-1]) / (values[i] - values[i-1]);
        return mix(colors[i-1], colors[i], f);
      }
    }

    return colors[VV_COLOR_N - 1];
  }
#endif
]]></snippet>

<snippet name="rgb2hsv"><![CDATA[
vec3 rgb2hsv(vec3 c)
{
  vec4 K = vec4(0.0, -1.0 / 3.0, 2.0 / 3.0, -1.0);
  vec4 p = mix(vec4(c.bg, K.wz), vec4(c.gb, K.xy), step(c.b, c.g));
  vec4 q = mix(vec4(p.xyw, c.r), vec4(c.r, p.yzx), step(p.x, c.r));

  float d = q.x - min(q.w, q.y);
  float e = 1.0e-10;
  return vec3(abs(q.z + (q.w - q.y) / (6.0 * d + e)), d / (q.x + e), q.x);
}
]]></snippet>

<snippet name="hsv2rgb"><![CDATA[
vec3 hsv2rgb(vec3 c)
{
  vec4 K = vec4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0);
  vec3 p = abs(fract(c.xxx + K.xyz) * 6.0 - K.www);
  return c.z * mix(K.xxx, clamp(p - K.xxx, 0.0, 1.0), c.y);
}
]]></snippet>

<snippet name="colorMixMode"><![CDATA[
$rgb2hsv
$hsv2rgb


/*
 * The color mix modes are encoded in the symbol color as follows:
 *  - Fully transparent symbols are represented with alpha 0 for
 *    all color mix modes (except ignore).
 *  - color mix mode ignore is encoded as multiply with white
 *  - the other 3 color mix modes (tint, replace, multiply) are
 *    equally distributed on the remaining 255 alpha values, which
 *    gives us 85 possible alpha values
 *
 * alpha             0 : fully transparent
 * alpha in [  1 -  85]: tint
 * alpha in [ 86 - 170]: replace
 * alpha in [171 - 255]: multiply
 */
vec4 decodeSymbolColor(vec4 symbolColor, out int colorMixMode) {
  float symbolAlpha = 0.0;

  const float maxTint = 85.0;
  const float maxReplace = 170.0;
  const float scaleAlpha = 3.0;

  if (symbolColor.a == 0.0) {
    colorMixMode = 1; // fully transparent -> multiply
    symbolAlpha = 0.0;
  }
  else if (symbolColor.a <= maxTint) {
    colorMixMode = 0; // tint
    symbolAlpha = scaleAlpha * symbolColor.a;
  }
  else if (symbolColor.a <= maxReplace) {
    colorMixMode = 3; // replace
    symbolAlpha = scaleAlpha * (symbolColor.a - maxTint);
  }
  else {
    colorMixMode = 1;  // multiply
    symbolAlpha = scaleAlpha * (symbolColor.a - maxReplace);
  }

  return vec4(symbolColor.rgb, symbolAlpha);
}

vec3 mixExternalColor(vec3 internalColor, vec3 textureColor, vec3 externalColor, int mode) {

  // workaround for artifacts in OSX using Intel Iris Pro
  // see: https://devtopia.esri.com/WebGIS/arcgis-js-api/issues/10475
  vec3 internalMixed = internalColor * textureColor;
  vec3 allMixed = internalMixed * externalColor;

  if (mode == 1 /* multiply */) {
    return allMixed;
  }
  else if (mode == 2 /* ignore */ ) {
    return internalMixed;
  }
  else if (mode == 3 /* replace */ ) {
    return externalColor;
  }
  else {
    // tint (or something invalid)
    vec3 hsvIn = rgb2hsv(internalMixed);
    vec3 hsvTint = rgb2hsv(externalColor);
    vec3 hsvOut = vec3(hsvTint.x, hsvTint.y, hsvIn.z * hsvTint.z);
    return hsv2rgb(hsvOut);
  }
}

float mixExternalOpacity(float internalOpacity, float textureOpacity, float externalOpacity, int mode) {

  // workaround for artifacts in OSX using Intel Iris Pro
  // see: https://devtopia.esri.com/WebGIS/arcgis-js-api/issues/10475
  float internalMixed = internalOpacity * textureOpacity;
  float allMixed = internalMixed * externalOpacity;

  if (mode == 2 /* ignore */ ) {
    return internalMixed;
  }
  else if (mode == 3 /* replace */ ) {
    return externalOpacity;
  }
  else {
    // multiply or tint (or something invalid)
    return allMixed;
  }
}

]]></snippet>

<snippet name="highlightWrite"><![CDATA[
  // the following uniforms are common to all highlight shaders:
  // uniform sampler2D depthTex
  // uniform vec4 highlightViewportPixelSz
  float sceneDepth = texture2D(depthTex, (gl_FragCoord.xy - highlightViewportPixelSz.xy) * highlightViewportPixelSz.zw).r;
  if (gl_FragCoord.z > sceneDepth + 5e-6) {
    gl_FragColor = vec4(1.0, 1.0, 0.0, 1.0);
  }
  else {
    gl_FragColor = vec4(1.0, 0.0, 1.0, 1.0);
  }
]]></snippet>

<snippet name="screenSizePerspective"><![CDATA[
#ifdef SCREEN_SIZE_PERSPECTIVE

// Note that the implementation here should be kept in sync with the corresponding
// CPU implementation (used for hitTest etc) in screenSizePerspectiveUtils.ts

/**
 * Compute the screen size perspective lower bound from pre-computed screen
 * size perspective factors (or parameters, since both store the pixel lower
 * bound information in the same place). When computing the minimum size,
 * the padding (e.g. text halo) is scaled with the same factor as the
 * original size scales to reach the minimum size.
 *
 * {
 *    x: N/A
 *    y: N/A
 *    z: minPixelSize (abs),
 *    w: sizePaddingInPixels (abs)
 * }
 */
float screenSizePerspectiveMinSize(float size, vec4 factor) {

  // Original calculation:
  //   padding = 2 * factor.w
  //   minSize = factor.z
  //
  //   minSize + minSize / size * padding
  //
  // Incorporates padding (factor.w, e.g. text halo size) into the
  // minimum bounds calculation, taking into account that padding
  // would scale down proportionally to the size.
  //
  // Calculation below is the same, but avoids division by zero when
  // size would be zero, without branching using step.
  // https://devtopia.esri.com/WebGIS/arcgis-js-api/issues/10683

  // nonZeroSize is 1 if size > 0, and 0 otherwise
  float nonZeroSize = 1.0 - step(size, 0.0);

  return (
    factor.z * (
      1.0 +
      nonZeroSize *                // Multiply by nzs ensures if size is 0, then we ignore
                                   // proportionally scaled padding
      2.0 * factor.w / (
        size + (1.0 - nonZeroSize) // Adding 1 - nzs ensures we divide either by size, or by 1
      )
    )
  );
}

/**
 * Computes the view angle dependent screen size perspective factor. The goal
 * of this factor is that:
 *
 *   1. There is no perspective when looking top-down
 *   2. There is a smooth and quick transition to full perspective when
 *      tilting.
 */
float screenSizePerspectiveViewAngleDependentFactor(float absCosAngle) {
  return absCosAngle * absCosAngle * absCosAngle;
}

/**
 * Precomputes a set of factors that can be used to apply screen size perspective
 * The factors are based on the viewing angle, distance to camera and the screen size
 * perspective parameters:
 * {
 *    x: distanceDivisor,
 *    y: distanceOffset,
 *    z: minPixelSize (abs),
 *    w: sizePaddingInPixels (abs)
 * }
 *
 * The result is a set of factors that can be used to apply the perspective:
 *
 * {
 *    x: distance based relative scale factor (0 -> 1)
 *    y: view dependent scale factor
 *    z: minPixelSize (abs)
 *    w: sizePaddingInPixels (abs)
 * }
 */
vec4 screenSizePerspectiveScaleFactor(float absCosAngle, float distanceToCamera, vec4 params) {
  return vec4(min(params.x / (distanceToCamera - params.y), 1.0), screenSizePerspectiveViewAngleDependentFactor(absCosAngle), params.z, params.w);
}

/**
 * Applies screen size perspective factors to a single dimension size, given the viewing angle,
 * distance to camera and perspective parameters. The factors can be calculated from the screen size
 * perspective parameters using screenSizePerspectiveScaleFactorFloat.
 *
 * Note that for single scale application, the screenSizePerspectiveScaleFloat can be used, which
 * will call this method, providing it the factors calculated from screenSizePerspectiveScaleFactorFloat.
 */

float applyScreenSizePerspectiveScaleFactorFloat(float size, vec4 factor) {
  return max(mix(size * factor.x, size, factor.y), screenSizePerspectiveMinSize(size, factor));
}

/**
 * Applies screen size perspective parameters to a single dimension size, given the viewing angle,
 * distance to camera and perspective parameters
 * {
 *    x: distanceDivisor,
 *    y: distanceOffset,
 *    z: minPixelSize (abs),
 *    w: sizePaddingInPixels (abs)
 * }
 */
float screenSizePerspectiveScaleFloat(float size, float absCosAngle, float distanceToCamera, vec4 params) {
  return applyScreenSizePerspectiveScaleFactorFloat(size, screenSizePerspectiveScaleFactor(absCosAngle, distanceToCamera, params));
}

/**
 * Applies screen size perspective factors to a vec2 size (width/height), given the viewing angle,
 * distance to camera and perspective parameters. The factors can be calculated from the screen size
 * perspective parameters using screenSizePerspectiveScaleFactorVec2.
 *
 * Note that for single scale application, the screenSizePerspectiveScaleVec2 can be used, which
 * will call this method, providing it the factors calculated from screenSizePerspectiveScaleFactorVec2.
 */
vec2 applyScreenSizePerspectiveScaleFactorVec2(vec2 size, vec4 factor) {
  return mix(size * clamp(factor.x, screenSizePerspectiveMinSize(size.y, factor) / size.y, 1.0), size, factor.y);
}

/**
 * Applies screen size perspective parameters to a vec2 size (width/height), given the viewing angle,
 * distance to camera and perspective parameters
 * {
 *    x: distanceDivisor,
 *    y: distanceOffset,
 *    z: minPixelSize (abs),
 *    w: sizePaddingInPixels (abs)
 * }
 */
vec2 screenSizePerspectiveScaleVec2(vec2 size, float absCosAngle, float distanceToCamera, vec4 params) {
  return applyScreenSizePerspectiveScaleFactorVec2(size, screenSizePerspectiveScaleFactor(absCosAngle, distanceToCamera, params));
}

#endif
]]></snippet>

<snippet name="computeNormal"><![CDATA[
  #ifdef GROUND_NORMAL_SHADING
    #ifdef VIEWING_MODE_GLOBAL
      vec3 normal = normalize(vpos + localOrigin);
    #else
      vec3 normal = vec3(0,0,1);
    #endif
  #else
    // compute normal
    #ifdef DOUBLESIDED
      vec3 normal = dot(vnormal, viewDir)>0.0 ? -vnormal : vnormal;
    #elif defined(WINDINGORDERDOUBLESIDED)
      vec3 normal = gl_FrontFacing ? vnormal : -vnormal;
    #else
      vec3 normal = vnormal;
    #endif
    normal = normalize(normal);
  #endif
]]></snippet>

<snippet name="decodeNormal"><![CDATA[
vec3 decodeNormal(vec2 f)
{
    float z = 1.0 - abs(f.x) - abs(f.y);
    return vec3(f + sign(f) * min(z, 0.0), z);
}
]]></snippet>

</snippets>
